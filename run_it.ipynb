{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a007fd0-5077-4082-95b9-a2e8171fe6f2",
   "metadata": {},
   "source": [
    "# Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50afb901-219f-4b81-8704-c2d686687e63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install tqdm matplotlib rasterio seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "217ad1e4-3940-4dc2-a588-a2ff4e6d2ad4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22ab7c3d-572e-4c63-962c-964b94453480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring reproducibility\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    \n",
    "    # cuDNN reproducibility settings\n",
    "    torch.backends.cudnn.benchmark = False \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.enabled = False\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be66d53f-3eca-4fcb-8007-fcd051dde019",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# We'll need this to send all the Pretraoinig, finetuning, and prediction processes into GPU\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1644ee49-0817-4b54-9adb-fe512a0d79e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "work_dir = \"./work_dir\"\n",
    "os.makedirs(work_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce01cc55-779b-4a48-9f67-52052a1794c8",
   "metadata": {},
   "source": [
    "# Necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce99b0ae-d567-4ca7-9e9f-220404fe77db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is the class that enables us to read the pickled dataset\n",
    "\n",
    "class PickledDataset(Dataset):\n",
    "    def __init__(self, imgs, labels=None, chip_indices=None, image_paths=None, metadata=None, usage=\"train\"):\n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "        self.chip_indices = chip_indices\n",
    "        self.image_paths = image_paths\n",
    "        self.metadata = metadata\n",
    "        self.usage = usage.lower()  # \"train\", or \"test\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = torch.from_numpy(self.imgs[idx].transpose(2, 0, 1)).float()\n",
    "\n",
    "        if self.usage == \"train\":\n",
    "            label = torch.from_numpy(self.labels[idx]).long()\n",
    "            return img, label\n",
    "\n",
    "        # test/prediction mode\n",
    "        sample = {\"image\": img}\n",
    "\n",
    "        if self.labels is not None:\n",
    "            sample[\"label\"] = torch.from_numpy(self.labels[idx]).long()\n",
    "\n",
    "        if self.chip_indices is not None:\n",
    "            sample[\"chip_index\"] = self.chip_indices[idx]\n",
    "\n",
    "        if self.image_paths is not None:\n",
    "            sample[\"image_path\"] = self.image_paths[0]\n",
    "\n",
    "        if self.metadata is not None:\n",
    "            sample[\"metadata\"] = self.metadata[0]\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "078933d7-9f5b-412f-a63b-78a59ba59d0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Metrics calculation\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "        self.confusion_matrix = np.zeros((num_classes, num_classes), dtype=np.int64)\n",
    "\n",
    "    def reset(self):\n",
    "        self.confusion_matrix[:] = 0\n",
    "\n",
    "    def add_batch(self, gt, pred):\n",
    "        gt = gt.flatten()\n",
    "        pred = pred.flatten()\n",
    "        mask = (gt >= 0) & (gt < self.num_classes)\n",
    "        self.confusion_matrix += np.bincount(\n",
    "            self.num_classes * gt[mask].astype(int) + pred[mask].astype(int),\n",
    "            minlength=self.num_classes ** 2\n",
    "        ).reshape(self.num_classes, self.num_classes)\n",
    "\n",
    "    def overall_accuracy(self):\n",
    "        return np.diag(self.confusion_matrix).sum() / np.maximum(self.confusion_matrix.sum(), 1)\n",
    "\n",
    "    def classwise_overall_accuracy(self):\n",
    "        return np.diag(self.confusion_matrix) / np.maximum(self.confusion_matrix.sum(axis=1), 1)\n",
    "\n",
    "    def intersection_over_union(self):\n",
    "        intersection = np.diag(self.confusion_matrix)\n",
    "        union = self.confusion_matrix.sum(axis=1) + self.confusion_matrix.sum(axis=0) - intersection\n",
    "        return intersection / np.maximum(union, 1)\n",
    "\n",
    "    def precision(self):\n",
    "        tp = np.diag(self.confusion_matrix)\n",
    "        fp = self.confusion_matrix.sum(axis=0) - tp\n",
    "        return tp / np.maximum(tp + fp, 1)\n",
    "\n",
    "    def recall(self):\n",
    "        tp = np.diag(self.confusion_matrix)\n",
    "        fn = self.confusion_matrix.sum(axis=1) - tp\n",
    "        return tp / np.maximum(tp + fn, 1)\n",
    "\n",
    "    def f1_score(self):\n",
    "        precision = self.precision()\n",
    "        recall = self.recall()\n",
    "        return 2 * precision * recall / np.maximum(precision + recall, 1e-7)\n",
    "\n",
    "    def plot_confusion_matrix(self, class_labels, unknown_class_idx=None):\n",
    "        cm = self.confusion_matrix.copy()\n",
    "        if unknown_class_idx is not None:\n",
    "            cm[unknown_class_idx, :] = 0\n",
    "            cm[:, unknown_class_idx] = 0\n",
    "\n",
    "        row_sums = cm.sum(axis=1, keepdims=True)\n",
    "        cm_normalized = np.divide(cm, row_sums, where=row_sums != 0) * 100\n",
    "        cm_normalized = np.nan_to_num(cm_normalized)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        sns.heatmap(\n",
    "            cm_normalized,\n",
    "            annot=True,\n",
    "            fmt=\".1f\",\n",
    "            cmap=\"Blues\",\n",
    "            xticklabels=class_labels,\n",
    "            yticklabels=class_labels,\n",
    "            ax=ax\n",
    "        )\n",
    "        ax.set_xlabel(\"Predicted\")\n",
    "        ax.set_ylabel(\"True\")\n",
    "        ax.set_title(\"Confusion Matrix (%)\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def evaluate_from_predictions(self, pickled_labels_path, pred_dir, class_mapping, unknown_class_idx, out_path):\n",
    "        def extract_tile_and_year(path):\n",
    "            base = os.path.basename(path)\n",
    "            parts = os.path.splitext(base)[0].split('_')\n",
    "            if len(parts) >= 2:\n",
    "                return parts[0], parts[1]\n",
    "            return None, None\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "        with open(pickled_labels_path, 'rb') as f:\n",
    "            label_dict = pickle.load(f)\n",
    "\n",
    "        label_map = {}\n",
    "        for idx, label_path in enumerate(label_dict[\"label_paths\"]):\n",
    "            tile, year = extract_tile_and_year(label_path)\n",
    "            if tile and year:\n",
    "                label_map[(tile, year)] = idx\n",
    "\n",
    "        pred_files = sorted([f for f in os.listdir(pred_dir) if f.endswith('.tif')])\n",
    "\n",
    "        for pred_file in pred_files:\n",
    "            tile, year = extract_tile_and_year(pred_file)\n",
    "            if not tile or not year:\n",
    "                print(f\"Skipping unmatched format: {pred_file}\")\n",
    "                continue\n",
    "\n",
    "            key = (tile, year)\n",
    "            if key not in label_map:\n",
    "                print(f\"No label found for prediction: {pred_file}\")\n",
    "                continue\n",
    "\n",
    "            idx = label_map[key]\n",
    "            ref_data = label_dict[\"chips\"][idx]\n",
    "            ref_data = np.squeeze(ref_data)\n",
    "\n",
    "            pred_path = os.path.join(pred_dir, pred_file)\n",
    "            with rasterio.open(pred_path) as src:\n",
    "                pred_data = src.read(1)\n",
    "\n",
    "            if ref_data.shape != pred_data.shape:\n",
    "                print(f\"Shape mismatch for {pred_file}\")\n",
    "                continue\n",
    "\n",
    "            self.add_batch(np.expand_dims(ref_data, axis=0), np.expand_dims(pred_data.astype(np.int32), axis=0))\n",
    "\n",
    "        acc = self.overall_accuracy()\n",
    "        iou = self.intersection_over_union()\n",
    "        precision = self.precision()\n",
    "        recall = self.recall()\n",
    "        f1 = self.f1_score()\n",
    "        classwise_acc = self.classwise_overall_accuracy()\n",
    "\n",
    "        metrics = {\n",
    "            \"Overall Accuracy\": acc,\n",
    "            \"Mean IoU\": np.nanmean(iou),\n",
    "            \"Mean Precision\": np.nanmean(precision),\n",
    "            \"Mean Recall\": np.nanmean(recall),\n",
    "            \"Mean F1 Score\": np.nanmean(f1)\n",
    "        }\n",
    "\n",
    "        with open(out_path, mode=\"w\", newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Metric\", \"Value\"])\n",
    "            for name, val in metrics.items():\n",
    "                writer.writerow([name, val])\n",
    "\n",
    "        classwise_path = Path(out_path).with_name(Path(out_path).stem + \"_classwise.csv\")\n",
    "        with open(classwise_path, mode=\"w\", newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Class\", \"Accuracy\", \"IoU\", \"Precision\", \"Recall\", \"F1 Score\"])\n",
    "            for i in range(self.num_classes):\n",
    "                if unknown_class_idx is not None and i == unknown_class_idx:\n",
    "                    continue\n",
    "                class_name = class_mapping.get(i, f\"class_{i}\")\n",
    "                writer.writerow([\n",
    "                    class_name,\n",
    "                    classwise_acc[i],\n",
    "                    iou[i],\n",
    "                    precision[i],\n",
    "                    recall[i],\n",
    "                    f1[i]\n",
    "                ])\n",
    "\n",
    "        label_names = [class_mapping.get(i, f\"class_{i}\") for i in range(self.num_classes)]\n",
    "        self.plot_confusion_matrix(label_names, unknown_class_idx)\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae48e5d5-be96-4879-84cd-1cb22577cfda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting function\n",
    "\n",
    "def plot_preds(merged_df, tile_indices, pred_dir, label_colors):\n",
    "    for tile_idx in tile_indices:\n",
    "        if tile_idx >= len(merged_df):\n",
    "            print(f\"Index {tile_idx} out of range.\")\n",
    "            continue\n",
    "\n",
    "        row = merged_df.iloc[tile_idx]\n",
    "        tile_name = row[\"label_path\"]\n",
    "        img = row[\"image_chip\"].astype(np.float32)\n",
    "        label = row[\"label_chip\"]\n",
    "\n",
    "        if img.max() > 1.5:\n",
    "            img /= img.max()\n",
    "\n",
    "        if img.shape[2] >= 3:\n",
    "            rgb = img[:, :, [2, 1, 0]]\n",
    "        else:\n",
    "            rgb = np.zeros((*img.shape[:2], 3), dtype=np.float32)\n",
    "\n",
    "        if img.shape[2] >= 4:\n",
    "            false = img[:, :, [3, 2, 1]]\n",
    "        else:\n",
    "            false = np.zeros_like(rgb)\n",
    "\n",
    "        pred_path = os.path.join(pred_dir, tile_name)\n",
    "        if not os.path.exists(pred_path):\n",
    "            print(f\"Prediction file not found for: {tile_name}\")\n",
    "            continue\n",
    "\n",
    "        with rasterio.open(pred_path) as src:\n",
    "            pred = src.read(1)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "        axes[0].imshow(np.clip(rgb, 0, 1))\n",
    "        axes[0].set_title(\"True Color (RGB)\")\n",
    "        axes[0].axis(\"off\")\n",
    "\n",
    "        axes[1].imshow(np.clip(false, 0, 1))\n",
    "        axes[1].set_title(\"False Color (NIR-R-G)\")\n",
    "        axes[1].axis(\"off\")\n",
    "\n",
    "        axes[2].imshow(label, cmap=label_colors, vmin=0, vmax=2)\n",
    "        axes[2].set_title(\"Reference Label\")\n",
    "        axes[2].axis(\"off\")\n",
    "\n",
    "        axes[3].imshow(pred, cmap=label_colors, vmin=0, vmax=2)\n",
    "        axes[3].set_title(\"Prediction\")\n",
    "        axes[3].axis(\"off\")\n",
    "\n",
    "        fig.suptitle(f\"Tile: {tile_name}\", fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f460f797-16c5-4a00-bb1d-0af6102f5c63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Our Unet class!\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout_rate=0.5):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=dropout_rate)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class TripleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout_rate=0.5):\n",
    "        super(TripleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=dropout_rate)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class unet_ra(nn.Module):\n",
    "    def __init__(self, n_classes, in_channels,\n",
    "                 filter_config=(64, 128, 256, 512, 1024, 1048), dropout_rate=0.5):\n",
    "        super(unet_ra, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.conv_block_enc_1 = DoubleConv(in_channels, filter_config[0], dropout_rate)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv_block_enc_2 = DoubleConv(filter_config[0], filter_config[1], dropout_rate)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv_block_enc_3 = DoubleConv(filter_config[1], filter_config[2], dropout_rate)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv_block_enc_4 = TripleConv(filter_config[2], filter_config[3], dropout_rate)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv_block_enc_5 = TripleConv(filter_config[3], filter_config[4], dropout_rate)\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Center\n",
    "        self.center_1 = Bottleneck(filter_config[4], filter_config[5])\n",
    "\n",
    "        # Decoder\n",
    "        self.up1 = nn.ConvTranspose2d(filter_config[5], filter_config[4], kernel_size=2, stride=2)\n",
    "        self.conv_block_dec_1 = TripleConv(filter_config[4]*2, filter_config[4], dropout_rate)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(filter_config[4], filter_config[3], kernel_size=2, stride=2)\n",
    "        self.conv_block_dec_2 = TripleConv(filter_config[3]*2, filter_config[3], dropout_rate)\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(filter_config[3], filter_config[2], kernel_size=2, stride=2)\n",
    "        self.conv_block_dec_3 = DoubleConv(filter_config[2]*2, filter_config[2], dropout_rate)\n",
    "\n",
    "        self.up4 = nn.ConvTranspose2d(filter_config[2], filter_config[1], kernel_size=2, stride=2)\n",
    "        self.conv_block_dec_4 = DoubleConv(filter_config[1]*2, filter_config[1], dropout_rate)\n",
    "\n",
    "        self.up5 = nn.ConvTranspose2d(filter_config[1], filter_config[0], kernel_size=2, stride=2)\n",
    "        self.conv_block_dec_5 = DoubleConv(filter_config[0]*2, filter_config[0], dropout_rate)\n",
    "\n",
    "        self.out = nn.Conv2d(filter_config[0], n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.conv_block_enc_1(x)\n",
    "        pool1 = self.pool1(enc1)\n",
    "\n",
    "        enc2 = self.conv_block_enc_2(pool1)\n",
    "        pool2 = self.pool2(enc2)\n",
    "\n",
    "        enc3 = self.conv_block_enc_3(pool2)\n",
    "        pool3 = self.pool3(enc3)\n",
    "\n",
    "        enc4 = self.conv_block_enc_4(pool3)\n",
    "        pool4 = self.pool4(enc4)\n",
    "\n",
    "        enc5 = self.conv_block_enc_5(pool4)\n",
    "        pool5 = self.pool5(enc5)\n",
    "\n",
    "        # Center\n",
    "        center = self.center_1(pool5)\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        up1 = self.up1(center)\n",
    "        dec1 = self.conv_block_dec_1(torch.cat([up1, enc5], dim=1))  #[batch_size, channels, height, width] dim=1 is channel dimention\n",
    "\n",
    "        up2 = self.up2(dec1)\n",
    "        dec2 = self.conv_block_dec_2(torch.cat([up2, enc4], dim=1))\n",
    "\n",
    "        up3 = self.up3(dec2)\n",
    "        dec3 = self.conv_block_dec_3(torch.cat([up3, enc3], dim=1))\n",
    "\n",
    "        up4 = self.up4(dec3)\n",
    "        dec4 = self.conv_block_dec_4(torch.cat([up4, enc2], dim=1))\n",
    "\n",
    "        up5 = self.up5(dec4)\n",
    "        dec5 = self.conv_block_dec_5(torch.cat([up5, enc1], dim=1))\n",
    "\n",
    "        out = self.out(dec5)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb5c5c1-80a8-414a-96a6-38989e067c4f",
   "metadata": {},
   "source": [
    "# Pretraining Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4b0f66-2431-41ec-a65e-17b1e3527b60",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 1: Load train/vaildation dataset pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d473230-c746-4fb7-b4ab-4e9eccefe984",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/airg/rabedi/UCSB_workshop'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47709c3c-f8dc-4839-990f-e00a1e173e40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reading train and validation dataset pickles\n",
    "\n",
    "with open(\"/home/airg/rabedi/UCSB/pickles/pretrain/train_data.pkl\", \"rb\") as f:\n",
    "    train_data = pickle.load(f)\n",
    "    train_imgs = train_data[\"images\"]\n",
    "    train_labels = train_data[\"labels\"]\n",
    "\n",
    "with open(\"/home/airg/rabedi/UCSB/pickles/pretrain/validate_data.pkl\", \"rb\") as f:\n",
    "    val_data = pickle.load(f)\n",
    "    val_imgs = val_data[\"images\"]\n",
    "    val_labels = val_data[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8866cfa0-1807-482e-817b-8f7a602dea42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create training and validation datasets\n",
    "\n",
    "train_dataset = PickledDataset(train_imgs, train_labels, usage=\"train\")\n",
    "val_dataset = PickledDataset(val_imgs, val_labels, usage=\"train\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=5, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b784f5c-aeaa-4cc9-9d82-af41fc09d6a4",
   "metadata": {},
   "source": [
    "### Step 2:  Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fec3614-6e1e-447a-90fb-6d6eb16acc26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "in_channels = 4 #train_imgs[0].shape[2]\n",
    "num_classes = 3 #len(set(label.max() for label in train_labels))\n",
    "num_epochs = 20\n",
    "class_mapping = {0: 'non_field', 1: 'field', 2: 'Boundary'}\n",
    "unknown_class_idx = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d1e490-2e82-4cc3-ae9c-ce41f988a9d5",
   "metadata": {},
   "source": [
    "### Step 3: Model initialization and hyperparameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15020eaf-640a-4dbf-9a13-c72bfa626738",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "\n",
    "model = unet_ra(n_classes=num_classes, in_channels=in_channels).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6999583f-5c32-4631-8c58-a410605f2e2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 4: Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc0e9d1-275f-4c71-b520-af0d5e3993a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_logs = []\n",
    "val_logs = []\n",
    "\n",
    "best_miou = 0.0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs} - Training...\")\n",
    "\n",
    "    train_bar = tqdm(enumerate(train_loader), total=len(train_loader), leave=False)\n",
    "    for i, (images, labels) in train_bar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_bar.set_description(f\"Batch {i + 1}\")\n",
    "        train_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    print(\"Validating...\")\n",
    "\n",
    "    val_bar = tqdm(val_loader, total=len(val_loader), leave=False)\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_bar:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "            val_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    evaluator = Evaluator(num_classes)\n",
    "    evaluator.add_batch(all_labels.cpu().numpy(), all_preds.cpu().numpy())\n",
    "\n",
    "    acc = evaluator.overall_accuracy()\n",
    "    miou = np.nanmean(evaluator.intersection_over_union())\n",
    "    precision = np.nanmean(evaluator.precision())\n",
    "    recall = np.nanmean(evaluator.recall())\n",
    "    f1 = np.nanmean(evaluator.f1_score())\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} Summary:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f}\")\n",
    "    print(f\"  mIoU:       {miou:.4f}\")\n",
    "    print(f\"  Accuracy:   {acc:.4f}\")\n",
    "    print(f\"  Precision:  {precision:.4f}\")\n",
    "    print(f\"  Recall:     {recall:.4f}\")\n",
    "    print(f\"  F1 Score:   {f1:.4f}\")\n",
    "\n",
    "    train_logs.append({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": train_loss\n",
    "    })\n",
    "\n",
    "    val_logs.append({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"miou\": miou,\n",
    "        \"accuracy\": acc,\n",
    "        \"mean_precision\": precision,\n",
    "        \"mean_recall\": recall,\n",
    "        \"mean_f1_score\": f1\n",
    "    })\n",
    "\n",
    "    if miou > best_miou:\n",
    "        best_miou = miou\n",
    "        best_model_state = model.state_dict()\n",
    "        print(f\"  Best model updated with validation mIoU {miou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749aed16-4ee1-40c4-ada4-98ba81e48a70",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 4: Saving logs/model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fcbbfd-a8a5-4e9a-9f8e-e214f012715a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save logs to CSV\n",
    "\n",
    "combined_logs = []\n",
    "for train, val in zip(train_logs, val_logs):\n",
    "    log = {\"epoch\": train[\"epoch\"], \"train_loss\": train[\"train_loss\"]}\n",
    "    log.update(val) \n",
    "    combined_logs.append(log)\n",
    "\n",
    "log_df = pd.DataFrame(combined_logs)\n",
    "log_df.to_csv(os.path.join(work_dir, \"pretrain_loss.csv\"), index=False)\n",
    "\n",
    "print(\"Training and validation logs saved to pretrain_loss.csv\")\n",
    "\n",
    "\n",
    "# Save best model (by mIoU)\n",
    "\n",
    "if best_model_state is not None:\n",
    "    torch.save(best_model_state, os.path.join(work_dir, \"pretrain_best_model.pth\"))\n",
    "    print(\"Best model saved to best_model.pth\")\n",
    "\n",
    "# Save final model (last epoch)\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(work_dir, \"pretrain_last_model.pth\"))\n",
    "print(\"Last model saved to last_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34718eec-eea5-4254-bc51-b7eaec98b426",
   "metadata": {},
   "source": [
    "### Step 5: Train and validation loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2924441-10e0-4c9b-88f8-6742016eaf4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_file = os.path.join(work_dir, \"pretrain_loss.csv\")\n",
    "\n",
    "df = pd.read_csv(log_file)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(df['epoch'], df['train_loss'], label='Train Loss', marker='o')\n",
    "plt.plot(df['epoch'], df['val_loss'], label='Val Loss', marker='o')\n",
    "plt.title(\"Train vs Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "metrics = df.columns.difference(['epoch', 'train_loss', 'val_loss'])\n",
    "num_metrics = len(metrics)\n",
    "cols = 2\n",
    "rows = math.ceil(num_metrics / cols)\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(12, 4 * rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    axes[idx].plot(df['epoch'], df[metric], marker='o', label=metric)\n",
    "    axes[idx].set_title(metric)\n",
    "    axes[idx].set_xlabel(\"Epoch\")\n",
    "    axes[idx].set_ylabel(metric)\n",
    "    axes[idx].grid(True)\n",
    "    axes[idx].legend()\n",
    "\n",
    "for j in range(idx + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d092d10-dd03-48a0-9cb4-9bdffb837845",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb6564d-83a3-40ab-ba53-145ea6c906dd",
   "metadata": {},
   "source": [
    "### Step 1: Load the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed90b27-d933-49ca-84b2-118d775bc5fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"./pickles/predict/test_data_img.pkl\", \"rb\") as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "test_imgs = test_data[\"chips\"]\n",
    "test_labels = test_data.get(\"labels\")\n",
    "chip_indices = test_data[\"chip_indices\"]\n",
    "image_paths = test_data[\"image_paths\"]\n",
    "metadata = test_data[\"metadata\"]\n",
    "tile_ids = test_data[\"tile_ids\"]\n",
    "years = test_data[\"years\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cecb3a1-afea-4cb8-bc86-b525281b4a1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = PickledDataset(\n",
    "    imgs=test_imgs,\n",
    "    labels=test_labels,                     \n",
    "    chip_indices=chip_indices,              \n",
    "    image_paths=image_paths,                \n",
    "    metadata=metadata,\n",
    "    usage=\"test\"\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aec753-ddf4-4ae3-8dca-0bdb8abb13e7",
   "metadata": {},
   "source": [
    "### Step 2: Load the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b12a1d-5d25-4d0e-bf01-2005c7c53dbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We assume that you didn't restart the kernel otherwise you'll need to initialize the model again\n",
    "\n",
    "model.load_state_dict(torch.load(os.path.join(work_dir, \"best_model.pth\"), map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cd2648-8317-424e-82f7-3f336bfbbbf0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 3: Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4520abff-f3c0-4325-a219-e7c6b75ffeb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_dir = os.path.join(work_dir, \"pred_2cls\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Group chips by tile\n",
    "grouped_data = defaultdict(lambda: {\n",
    "    \"chips\": [],\n",
    "    \"chip_indices\": [],\n",
    "    \"meta\": None,\n",
    "    \"image_path\": None\n",
    "})\n",
    "\n",
    "for chip, idx, meta, path, tile in zip(test_imgs, chip_indices, metadata, image_paths, tile_ids):\n",
    "    grouped_data[tile][\"chips\"].append(chip)\n",
    "    grouped_data[tile][\"chip_indices\"].append(idx)\n",
    "    grouped_data[tile][\"meta\"] = meta\n",
    "    grouped_data[tile][\"image_path\"] = path\n",
    "\n",
    "# === Predict and save each tile ===\n",
    "for tile_id, data in tqdm(grouped_data.items(), desc=\"Predicting tiles\"):\n",
    "    meta = data[\"meta\"].copy()\n",
    "    meta.update({\n",
    "        \"count\": 1,\n",
    "        \"dtype\": rasterio.int8\n",
    "    })\n",
    "    meta.pop(\"nodata\", None)  # remove if invalid\n",
    "\n",
    "    tile_height = meta[\"height\"]\n",
    "    tile_width = meta[\"width\"]\n",
    "    full_pred = np.zeros((tile_height, tile_width), dtype=np.int8)\n",
    "\n",
    "    for chip, (chip_row, chip_col) in zip(data[\"chips\"], data[\"chip_indices\"]):\n",
    "        img = torch.from_numpy(chip.transpose(2, 0, 1)).unsqueeze(0).float().to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(img)\n",
    "\n",
    "            pred = output.argmax(dim=1).squeeze().cpu().numpy().astype(np.int8)\n",
    "\n",
    "            # Convert 3-class prediction → binary:\n",
    "            # 0 = non-field, 1 = field and boundary (original classes 1 and 2)\n",
    "            pred = np.where(pred == 2, 1, pred)\n",
    "\n",
    "        chip_size = pred.shape[0]\n",
    "        full_pred[\n",
    "            chip_row:chip_row + chip_size,\n",
    "            chip_col:chip_col + chip_size\n",
    "        ] = pred\n",
    "\n",
    "    tile_path = data[\"image_path\"]\n",
    "    tile_name = os.path.splitext(os.path.basename(tile_path))[0]\n",
    "    pred_path = os.path.join(save_dir, f\"{tile_name}.tif\")\n",
    "\n",
    "    with rasterio.open(pred_path, \"w\", **meta) as dst:\n",
    "        dst.write(full_pred, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4876cc-a525-42c0-a7da-9f034038eb58",
   "metadata": {},
   "source": [
    "### Step 4: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065fb223-89f6-40c4-b5c4-1d044f2889a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load pickled image and label data\n",
    "\n",
    "with open(\"./pickles/predict/test_data_img.pkl\", \"rb\") as f:\n",
    "    img_data = pickle.load(f)\n",
    "\n",
    "with open(\"./pickles/predict/test_data_lbl_2cls.pkl\", \"rb\") as f:\n",
    "    lbl_data = pickle.load(f)\n",
    "\n",
    "img_df = pd.DataFrame({\n",
    "    \"chip_index\": img_data[\"chip_indices\"],\n",
    "    \"tile_id\": img_data[\"tile_ids\"],\n",
    "    \"year\": img_data[\"years\"],\n",
    "    \"image_chip\": img_data[\"chips\"]\n",
    "})\n",
    "\n",
    "lbl_df = pd.DataFrame({\n",
    "    \"chip_index\": lbl_data[\"chip_indices\"],\n",
    "    \"tile_id\": lbl_data[\"tile_ids\"],\n",
    "    \"year\": lbl_data[\"years\"],\n",
    "    \"label_chip\": lbl_data[\"chips\"],\n",
    "    \"label_path\": lbl_data[\"label_paths\"]\n",
    "})\n",
    "\n",
    "merged_df = pd.merge(\n",
    "    img_df,\n",
    "    lbl_df,\n",
    "    on=[\"chip_index\", \"tile_id\", \"year\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1fddc7-165d-47fe-84b7-d97d95e9beeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calling plot function\n",
    "\n",
    "plot_preds(\n",
    "    merged_df=merged_df,\n",
    "    tile_indices=[0, 2],\n",
    "    pred_dir=\"./work_dir/pred_2cls\",\n",
    "    label_colors=ListedColormap([\"#FFFF00\", \"#00FF00\", \"#000000\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bbdaac-ef6e-4095-a28b-b573c0f870d6",
   "metadata": {},
   "source": [
    "### Step 5: Accuracy Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42d0e1c-a7ec-43da-91ee-228ee5faad2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the evaluator\n",
    "\n",
    "evaluator = Evaluator(num_classes=2)\n",
    "\n",
    "# Configuration\n",
    "pickled_labels_path = \"./pickles/predict/test_data_lbl_2cls.pkl\"\n",
    "pred_dir = \"./work_dir/pred_2cls\"\n",
    "unknown_class_idx = None\n",
    "class_mapping = {0: 'non_field', 1: 'field'} \n",
    "out_path = \"./work_dir/eval_metrics_train_only.csv\"\n",
    "\n",
    "# Run evaluation\n",
    "evaluator.evaluate_from_predictions(\n",
    "    pickled_labels_path=pickled_labels_path,\n",
    "    pred_dir=pred_dir,\n",
    "    class_mapping=class_mapping,\n",
    "    unknown_class_idx=unknown_class_idx,\n",
    "    out_path=out_path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa43026-5dcb-430e-95da-9d2c257f9055",
   "metadata": {},
   "source": [
    "# Finetuning Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269b85e1-ee30-4a5a-af68-28d55082af80",
   "metadata": {},
   "source": [
    "### Step 1: Loading finetune dataset pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c6985a-4eae-4e29-a190-983b4b625410",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"./pickles/finetune/train_data.pkl\", \"rb\") as f:\n",
    "    train_data = pickle.load(f)\n",
    "    train_imgs = train_data[\"images\"]\n",
    "    train_labels = train_data[\"labels\"]\n",
    "\n",
    "with open(\"./pickles/finetune/validate_data.pkl\", \"rb\") as f:\n",
    "    val_data = pickle.load(f)\n",
    "    val_imgs = val_data[\"images\"]\n",
    "    val_labels = val_data[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6d9ed2-c36b-4d5f-82a6-46cefb984189",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create training and validation datasets\n",
    "\n",
    "train_dataset = PickledDataset(train_imgs, train_labels, usage=\"train\")\n",
    "val_dataset = PickledDataset(val_imgs, val_labels, usage=\"train\")\n",
    "\n",
    "# Create dataloaders\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=5, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97efc648-954c-4e65-90ad-2651086bb448",
   "metadata": {},
   "source": [
    "### Step 2: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c7953c-5fc9-467f-842e-7d4ea9227d22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load pretrained weights\n",
    "\n",
    "checkpoint_path = \"./work_dir/pretrain_best_model.pth\"\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "print(\"Loaded pretrained model weights.\")\n",
    "\n",
    "# Freeze layers by index\n",
    "\n",
    "freeze_indices = list(range(7))  # You can adjust based on which layers you want to freeze!\n",
    "modules = list(model.children())\n",
    "for i in freeze_indices:\n",
    "    for param in modules[i].parameters():\n",
    "        param.requires_grad = False\n",
    "    print(f\"Froze layer {i}: {modules[i].__class__.__name__}\")\n",
    "\n",
    "# Set up optimizer for trainable parameters\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=0.001,\n",
    "    weight_decay=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1d476c-8e1b-458a-8484-6c01a6063239",
   "metadata": {},
   "source": [
    "### Step 3: Training (finetuning) loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c998fa-90ef-4701-91bd-824d16c2053d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_logs = []\n",
    "val_logs = []\n",
    "best_miou = 0.0\n",
    "best_model_state = None\n",
    "num_epochs = 40\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs} - Training\")\n",
    "\n",
    "    train_bar = tqdm(enumerate(train_loader), total=len(train_loader), leave=False)\n",
    "    for i, (images, labels) in train_bar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_bar.set_description(f\"Batch {i + 1}\")\n",
    "        train_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    print(\"Validating\")\n",
    "\n",
    "    val_bar = tqdm(val_loader, total=len(val_loader), leave=False)\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_bar:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "            val_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    evaluator = Evaluator(num_classes)\n",
    "    evaluator.add_batch(all_labels.cpu().numpy(), all_preds.cpu().numpy())\n",
    "\n",
    "    miou = np.nanmean(evaluator.intersection_over_union())\n",
    "    acc = evaluator.overall_accuracy()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} Summary:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f}\")\n",
    "    print(f\"  mIoU:       {miou:.4f}\")\n",
    "    print(f\"  Accuracy:   {acc:.4f}\")\n",
    "\n",
    "    train_logs.append({\"epoch\": epoch + 1, \"train_loss\": train_loss})\n",
    "    val_logs.append({\"epoch\": epoch + 1, \"val_loss\": val_loss, \"miou\": miou, \"accuracy\": acc})\n",
    "\n",
    "    if miou > best_miou:\n",
    "        best_miou = miou\n",
    "        best_model_state = model.state_dict()\n",
    "        print(f\"Best model updated with validation mIoU {miou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8ef6e8-0073-4c67-9ac1-7e19653b7a8c",
   "metadata": {},
   "source": [
    "### Step 4: Saving logs/model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb6daad-f294-4e9d-bb22-519c6da5fe27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_logs = []\n",
    "for train, val in zip(train_logs, val_logs):\n",
    "    log = {\"epoch\": train[\"epoch\"], \"train_loss\": train[\"train_loss\"], \"val_loss\": val[\"val_loss\"], \"miou\": val[\"miou\"], \"accuracy\": val[\"accuracy\"]}\n",
    "    combined_logs.append(log)\n",
    "\n",
    "log_df = pd.DataFrame(combined_logs)\n",
    "log_df.to_csv(os.path.join(work_dir, \"finetune_loss.csv\"), index=False)\n",
    "\n",
    "print(\"Fine-tuning logs saved to finetune_loss.csv\")\n",
    "\n",
    "\n",
    "# Save best model (by mIoU)\n",
    "\n",
    "if best_model_state is not None:\n",
    "    torch.save(best_model_state, os.path.join(work_dir, \"finetuned_best_model.pth\"))\n",
    "    print(\"Best model saved to finetuned_best_model.pth\")\n",
    "\n",
    "# Save final model (last epoch)\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(work_dir, \"finetuned_last_model.pth\"))\n",
    "print(\"Last model saved to finetuned_last_model.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13525197-1898-46f2-ba3d-c61bd5bd229e",
   "metadata": {},
   "source": [
    "### Step 5: Train and validation loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b88dcc-87d6-4aab-89e0-41edae22c387",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = os.path.join(work_dir, \"finetune_loss.csv\")\n",
    "\n",
    "df = pd.read_csv(log_file)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(df['epoch'], df['train_loss'], label='Train Loss', marker='o')\n",
    "plt.plot(df['epoch'], df['val_loss'], label='Val Loss', marker='o')\n",
    "plt.title(\"Train vs Val Loss (Finetuning)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "metrics = df.columns.difference(['epoch', 'train_loss', 'val_loss'])\n",
    "num_metrics = len(metrics)\n",
    "cols = 2\n",
    "rows = math.ceil(num_metrics / cols)\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(12, 4 * rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    axes[idx].plot(df['epoch'], df[metric], marker='o', label=metric)\n",
    "    axes[idx].set_title(metric)\n",
    "    axes[idx].set_xlabel(\"Epoch\")\n",
    "    axes[idx].set_ylabel(metric)\n",
    "    axes[idx].grid(True)\n",
    "    axes[idx].legend()\n",
    "\n",
    "for j in range(idx + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb09237-0cc3-4d6d-ae5b-4a005bea201c",
   "metadata": {},
   "source": [
    "# Prediction after finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e138a2f6-ffdc-4e16-9f05-aafa71d41823",
   "metadata": {},
   "source": [
    "### Step 1: Load the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a85812-07d6-4acc-8966-354c4b1d678b",
   "metadata": {},
   "source": [
    "Same as previous!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3cefe9-a99e-4a03-b3bb-422741becb17",
   "metadata": {},
   "source": [
    "### Step 2: Load the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dcf6df-1160-4bbb-a3c2-3624631c436c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We assume that you didn't restart the kernel otherwise you'll need to initialize the model again\n",
    "\n",
    "model.load_state_dict(torch.load(os.path.join(work_dir, \"finetuned_best_model.pth\"), map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb94817-6136-49fc-9e71-44f040fedc4a",
   "metadata": {},
   "source": [
    "### Step 3: Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69724d7-574d-431d-9297-e967ee88a4eb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_dir = os.path.join(work_dir, \"pred_with_finetune_2cls\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Group chips by tile\n",
    "grouped_data = defaultdict(lambda: {\n",
    "    \"chips\": [],\n",
    "    \"chip_indices\": [],\n",
    "    \"meta\": None,\n",
    "    \"image_path\": None\n",
    "})\n",
    "\n",
    "for chip, idx, meta, path, tile in zip(test_imgs, chip_indices, metadata, image_paths, tile_ids):\n",
    "    grouped_data[tile][\"chips\"].append(chip)\n",
    "    grouped_data[tile][\"chip_indices\"].append(idx)\n",
    "    grouped_data[tile][\"meta\"] = meta\n",
    "    grouped_data[tile][\"image_path\"] = path\n",
    "\n",
    "# Predict and save each tile\n",
    "for tile_id, data in tqdm(grouped_data.items(), desc=\"Predicting tiles\"):\n",
    "    meta = data[\"meta\"].copy()\n",
    "    meta.update({\n",
    "        \"count\": 1,\n",
    "        \"dtype\": rasterio.int8\n",
    "    })\n",
    "    meta.pop(\"nodata\", None)\n",
    "\n",
    "    tile_height = meta[\"height\"]\n",
    "    tile_width = meta[\"width\"]\n",
    "    full_pred = np.zeros((tile_height, tile_width), dtype=np.int8)\n",
    "\n",
    "    for chip, (chip_row, chip_col) in zip(data[\"chips\"], data[\"chip_indices\"]):\n",
    "        img = torch.from_numpy(chip.transpose(2, 0, 1)).unsqueeze(0).float().to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(img)\n",
    "            pred = output.argmax(dim=1).squeeze().cpu().numpy().astype(np.int8)\n",
    "\n",
    "            # Convert 3-class prediction → binary:\n",
    "            # 0 = non-field, 1 = field and boundary (original classes 1 and 2)\n",
    "            pred = np.where(pred == 2, 1, pred)\n",
    "\n",
    "        chip_size = pred.shape[0]\n",
    "        full_pred[\n",
    "            chip_row:chip_row + chip_size,\n",
    "            chip_col:chip_col + chip_size\n",
    "        ] = pred\n",
    "\n",
    "    tile_path = data[\"image_path\"]\n",
    "    tile_name = os.path.splitext(os.path.basename(tile_path))[0]\n",
    "    pred_path = os.path.join(save_dir, f\"{tile_name}.tif\")\n",
    "\n",
    "    with rasterio.open(pred_path, \"w\", **meta) as dst:\n",
    "        dst.write(full_pred, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da64e48d-1cdf-45aa-b05b-5b9704dcaffa",
   "metadata": {},
   "source": [
    "### Step 4: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e6e106-afe1-4f58-aeb0-5d28e6b4a9e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load pickled image and label data (the same as before)\n",
    "\n",
    "with open(\"./pickles/predict/test_data_img.pkl\", \"rb\") as f:\n",
    "    img_data = pickle.load(f)\n",
    "\n",
    "with open(\"./pickles/predict/test_data_lbl_2cls.pkl\", \"rb\") as f:\n",
    "    lbl_data = pickle.load(f)\n",
    "\n",
    "img_df = pd.DataFrame({\n",
    "    \"chip_index\": img_data[\"chip_indices\"],\n",
    "    \"tile_id\": img_data[\"tile_ids\"],\n",
    "    \"year\": img_data[\"years\"],\n",
    "    \"image_chip\": img_data[\"chips\"]\n",
    "})\n",
    "\n",
    "lbl_df = pd.DataFrame({\n",
    "    \"chip_index\": lbl_data[\"chip_indices\"],\n",
    "    \"tile_id\": lbl_data[\"tile_ids\"],\n",
    "    \"year\": lbl_data[\"years\"],\n",
    "    \"label_chip\": lbl_data[\"chips\"],\n",
    "    \"label_path\": lbl_data[\"label_paths\"]\n",
    "})\n",
    "\n",
    "merged_df = pd.merge(\n",
    "    img_df,\n",
    "    lbl_df,\n",
    "    on=[\"chip_index\", \"tile_id\", \"year\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3715209-d321-4f87-91b1-6a517145eb66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calling plot function\n",
    "\n",
    "plot_preds(\n",
    "    merged_df=merged_df,\n",
    "    tile_indices=[0, 2],\n",
    "    pred_dir=\"./work_dir/pred_with_finetune_2cls\",\n",
    "    label_colors=ListedColormap([\"#FFFF00\", \"#00FF00\", \"#000000\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33b13fd-4652-4e36-8a92-59e9937cc91e",
   "metadata": {},
   "source": [
    "### Step 5: Acuracy assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca63cd3-8115-40f7-b6cd-1f38df57b15f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the evaluator\n",
    "\n",
    "evaluator = Evaluator(num_classes=2)\n",
    "\n",
    "# Configuration\n",
    "pickled_labels_path = \"./pickles/predict/test_data_lbl_2cls.pkl\"\n",
    "pred_dir = \"./work_dir/pred_with_finetune_2cls\"\n",
    "unknown_class_idx = None\n",
    "class_mapping = {0: 'non_field', 1: 'field'} \n",
    "out_path = \"./work_dir/eval_metrics_finetune.csv\"\n",
    "\n",
    "# Run evaluation\n",
    "evaluator.evaluate_from_predictions(\n",
    "    pickled_labels_path=pickled_labels_path,\n",
    "    pred_dir=pred_dir,\n",
    "    class_mapping=class_mapping,\n",
    "    unknown_class_idx=unknown_class_idx,\n",
    "    out_path=out_path\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

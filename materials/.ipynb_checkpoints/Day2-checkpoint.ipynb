{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9c8262a-7bb2-4184-ac18-c74f4cfdb811",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Transfer Learning and Generalization Capacity\n",
    "\n",
    "\n",
    "What is Domain Shift?\n",
    "\n",
    "**Domain shift** occurs when the training data (source domain) and testing or deployment data (target domain) come from different distributions—affecting the model’s performance.\n",
    "\n",
    "**Types of domain shift include:**\n",
    "- **Covariate shift**: Input distribution changes but label function remains (e.g., satellite images under different lighting conditions).\n",
    "- **Label shift**: Class proportions differ (e.g., more rice fields in one region).\n",
    "- **Concept shift**: The actual definition of a class changes across domains (e.g., crop labels depend on local farming systems).\n",
    "\n",
    "> *Torralba & Efros, 2011 – \"Unbiased look at dataset bias\"*  \n",
    "> https://doi.org/10.1109/CVPR.2011.5995347\n",
    "\n",
    "\n",
    "### Generalization Capacity\n",
    "\n",
    "**Generalization** refers to a model’s ability to perform well on unseen domains or tasks. A model with strong generalization:\n",
    "- Learns domain-invariant or semantically rich features.\n",
    "- Does not overfit to dataset-specific noise or bias.\n",
    "\n",
    "**Example**: A crop classification model trained on Europe performs well in Zambia **without retraining**—this indicates good generalization.\n",
    "\n",
    "> *Recht et al., 2019 – \"Do ImageNet Classifiers Generalize to ImageNet?\"*  \n",
    "> https://arxiv.org/abs/1902.10811\n",
    "\n",
    "---\n",
    "\n",
    "### Common Adaptation Strategies\n",
    "\n",
    "#### Domain-Level Adaptation\n",
    "\n",
    "**Definition**: Aligns the distributions of source and target domains globally.\n",
    "\n",
    "**Example**: Adapting a model trained on Sentinel-2 (Europe) to Landsat-8 (Africa).\n",
    "\n",
    "**Popular Methods**:\n",
    "- Domain-Adversarial Training (DANN)  \n",
    "> *Ganin et al., 2016 – \"Domain-Adversarial Training of Neural Networks\"*  \n",
    "> https://arxiv.org/abs/1505.07818\n",
    "\n",
    "\n",
    "\n",
    "#### Instance-Level Adaptation\n",
    "\n",
    "**Definition**: Focuses on adapting at the individual sample level.\n",
    "\n",
    "**Example**: Test-time adaptation using a few images from a new location to adjust predictions.\n",
    "\n",
    "**Popular Methods**:\n",
    "- Tent (Test-time Entropy Minimization)  \n",
    "> *Wang et al., 2021 – \"Tent: Fully Test-Time Adaptation by Entropy Minimization\"*  \n",
    "> https://arxiv.org/abs/2006.10726\n",
    "\n",
    "#### Task Adaptation\n",
    "\n",
    "**Definition**: Transfers knowledge between different tasks—e.g., from classification to segmentation.\n",
    "\n",
    "**Example**: Using a pretrained encoder from a crop classifier to initialize a segmentation model for field boundaries.\n",
    "\n",
    "**Popular Methods**:\n",
    "- Taskonomy  \n",
    "> *Zamir et al., 2018 – \"Taskonomy: Disentangling Task Transfer Learning\"*  \n",
    "> https://arxiv.org/abs/1804.08328\n",
    "\n",
    "\n",
    "#### Feature-Level Adaptation\n",
    "\n",
    "**Definition**: Matches intermediate feature distributions across domains.\n",
    "\n",
    "**Example**: Aligning feature embeddings from spring and summer images of the same region.\n",
    "\n",
    "**Popular Methods**:\n",
    "- Maximum Mean Discrepancy (MMD)  \n",
    "> *Long et al., 2015 – \"Learning Transferable Features with Deep Adaptation Networks\"*  \n",
    "> https://arxiv.org/abs/1502.02791\n",
    "\n",
    "#### Representation Adaptation\n",
    "\n",
    "**Definition**: Learns general-purpose features through self-supervised or contrastive pretraining.\n",
    "\n",
    "**Example**: Pretraining a masked autoencoder on multi-sensor satellite data to learn robust spatial-temporal patterns.\n",
    "\n",
    "**Popular Methods**:\n",
    "- MAE (Masked Autoencoder)  \n",
    "> *He et al., 2022 – \"Masked Autoencoders Are Scalable Vision Learners\"*  \n",
    "> https://arxiv.org/abs/2111.06377\n",
    "\n",
    "#### Label Space Adaptation\n",
    "\n",
    "**Definition**: Aligns source and target domains with **different or partially overlapping labels**.\n",
    "\n",
    "**Example**: Source domain has 10 crop classes; target only has 5.\n",
    "\n",
    "**Popular Methods**:\n",
    "- Partial Domain Adaptation (PADA)  \n",
    "> *Cao et al., 2018 – \"Partial Adversarial Domain Adaptation\"*  \n",
    "> https://arxiv.org/abs/1707.07901\n",
    "\n",
    "#### Conditional Adaptation\n",
    "\n",
    "**Definition**: Performs alignment **conditioned on class or domain-specific information**.\n",
    "\n",
    "**Example**: Aligning \"rice\" features between India and China, while ignoring background class differences.\n",
    "\n",
    "**Popular Methods**:\n",
    "- Conditional Adversarial Domain Adaptation (CDAN)  \n",
    "> *Long et al., 2018 – \"Conditional Adversarial Domain Adaptation\"*  \n",
    "> https://arxiv.org/abs/1705.10667\n",
    "\n",
    "#### Model Architecture Adaptation\n",
    "\n",
    "**Definition**: Modifies architectural components (e.g., normalization layers) to better handle domain variability.\n",
    "\n",
    "**Example**: Replacing batch norm with instance norm to reduce domain sensitivity in satellite images.\n",
    "\n",
    "**Popular Methods**:\n",
    "- AdaBN (Adaptive Batch Normalization)  \n",
    "> *Li et al., 2016 – \"Revisiting Batch Normalization for Practical Domain Adaptation\"*  \n",
    "> https://arxiv.org/abs/1603.04779\n",
    "\n",
    "#### Multi-Source Adaptation\n",
    "\n",
    "**Definition**: Adapts from **multiple diverse source domains** to one target domain.\n",
    "\n",
    "**Example**: Combining data from Europe, Asia, and the US to build a single model for Africa.\n",
    "\n",
    "**Popular Methods**:\n",
    "- MDAN (Multi-Source Domain Adaptation Network)  \n",
    "> *Zhao et al., 2018 – \"Adversarial Multiple Source Domain Adaptation\"*  \n",
    "> https://arxiv.org/abs/1809.02254\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a56d6ac-7a43-485c-85ce-9d84a2bbeea0",
   "metadata": {},
   "source": [
    "## Bonus Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df389588-0052-47c7-9625-a86b4908e195",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## GFM examples: FlexiMo, A Flexible Foundation Model for Multi-Resolution Remote Sensing\n",
    "\n",
    "**FlexiMo** is a remote sensing foundation model designed to address the challenges of applying vision transformers (ViTs) to images captured at arbitrary spatial resolutions and spectral configurations.\n",
    "\n",
    "Unlike traditional ViTs, which assume fixed patch sizes and consistent input dimensions, FlexiMo introduces architectural innovations that allow it to handle variable resolutions, dimensions, and channel counts — essential in Earth observation where data is heterogeneous across sensors, scales, and tasks.\n",
    "\n",
    "\n",
    "### Challenges in Vision Transformers (ViTs) for Remote Sensing\n",
    "\n",
    "**Rigid Tokenization Mechanism**  \n",
    "  Standard ViTs require fixed-length tokens for positional encoding. This constraint limits flexibility and may prevent the model from capturing spatial detail accurately when image dimensions vary.\n",
    "\n",
    "**Multi-Scale Perception Conflict**  \n",
    "  Fixed patch sizes across datasets with different spatial resolutions cause inconsistent real-world coverage per patch. The same object may appear at different scales, leading to semantically inconsistent inputs and poor generalization.\n",
    "\n",
    "\n",
    "### FlexiMo Architecture\n",
    "\n",
    "FlexiMo addresses these issues through two core modules:\n",
    "\n",
    "\n",
    "1. Spatial Resolution-Aware Module (SRAM)\n",
    "\n",
    "This module enables dimensional independence and resolution flexibility through:\n",
    "\n",
    "- **Dynamic Patch Size Adaptation**  \n",
    "  Instead of using a fixed patch size (e.g., 16×16), the module adapts patch size \\( P \\) based on the input image’s native resolution.  \n",
    "  - High-resolution images → larger patches (to reduce computation)  \n",
    "  - Low-resolution images → smaller patches (to preserve detail)\n",
    "\n",
    "- **Preservation of Embedding Properties via Pseudo-Inverse Bilinear Interpolation**  \n",
    "  To avoid distortion of token features, FlexiMo uses a pseudo-inverse of bilinear interpolation rather than traditional resizing.  \n",
    "  This ensures token norms and structural relationships are preserved, which is essential for scale-consistent representations.\n",
    "\n",
    "- **Multi-Scale Feature Extraction**  \n",
    "  The flexible tokenization allows the model to extract fine-grained and coarse-grained representations simultaneously, improving generalization across spatial resolutions and downstream tasks.\n",
    "\n",
    "Reference implementation detail: Images are input as \\( I \\), patch size parameter \\( P \\), and electromagnetic wavelength parameters \\( \\{\\lambda_i\\}_{i=1}^C \\).\n",
    "\n",
    "2. Channel Adaptation Module\n",
    "\n",
    "FlexiMo supports input from sensors with varying spectral characteristics by:\n",
    "\n",
    "- Leveraging prior knowledge of electromagnetic wavelengths to guide channel adaptation.\n",
    "- Dynamically recalibrating the input channels to preserve physical consistency and semantic coherence across spectral bands.\n",
    "- This makes FlexiMo well-suited for processing data from multi-spectral and hyperspectral sensors with inconsistent band counts.\n",
    "\n",
    "\n",
    "\n",
    "FlexiMo brings transformer-based representation learning to remote sensing with unmatched flexibility:\n",
    "\n",
    "- Adapts to arbitrary spatial resolutions\n",
    "- Preserves embedding integrity across scales\n",
    "- Learns resolution- and sensor-invariant representations\n",
    "- Processes images with non-uniform spectral channels\n",
    "\n",
    "These innovations make it a robust and generalizable choice for Earth observation tasks, including classification, segmentation, and change detection, across diverse satellite platforms.\n",
    "\n",
    "\n",
    "![Fleximo](https://github.com/Rahebe22/UCSB_workshop/raw/main/materials/figures/fleximo.png)\n",
    "\n",
    "\n",
    "Li, Xuyang, et al. *FlexiMo: A Flexible Remote Sensing Foundation Model*. arXiv preprint arXiv:2503.23844, 2025. [https://arxiv.org/abs/2503.23844](https://arxiv.org/abs/2503.23844)\n",
    "\n",
    "\n",
    "\n",
    "[Prithvi-v1](https://huggingface.co/ibm-nasa-geospatial/Prithvi-EO-1.0-100M)\n",
    "\n",
    "[Prithvi-v2](https://huggingface.co/ibm-nasa-geospatial/Prithvi-EO-2.0-600M)\n",
    "\n",
    "\n",
    "\n",
    "[Galileo](https://github.com/nasaharvest/galileo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a73c8d-213b-442e-90a3-40c97903ffe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
